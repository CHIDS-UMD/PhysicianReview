{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bda6e2d",
   "metadata": {},
   "source": [
    "# Doctor List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4916b60f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-07T18:37:50.252491Z",
     "start_time": "2021-08-07T18:37:49.923153Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a112ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'Data/.p'\n",
    "\n",
    "MD_Doc = pd.read_pickle(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc5cbef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-07T18:37:50.588120Z",
     "start_time": "2021-08-07T18:37:50.253717Z"
    }
   },
   "outputs": [],
   "source": [
    "path = 'Data/MD_DocListDF_All.p'\n",
    "\n",
    "MD_Doc = pd.read_pickle(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a124bae7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-07T18:38:10.885881Z",
     "start_time": "2021-08-07T18:38:10.852580Z"
    }
   },
   "outputs": [],
   "source": [
    "len(MD_Doc['NPI'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ab2ad6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d871f80",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T22:36:57.589671Z",
     "start_time": "2021-07-31T22:36:57.358906Z"
    }
   },
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "# db_connection_str = 'mysql+pymysql://root:@localhost:3306/doctorinfo_sample?charset=utf8'\n",
    "# db_connection = create_engine(db_connection_str)\n",
    "\n",
    "# df = pd.read_sql('SELECT * FROM physicians_sample', con=db_connection)\n",
    "\n",
    "\n",
    "# df.to_pickle('DocListDF.p')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_doctor_info(DoctorDF):\n",
    "\n",
    "\n",
    "    doctor_infolist = []\n",
    "    for idx, row in DoctorDF.iterrows():\n",
    "\n",
    "        # most results: fistname is before lastname\n",
    "        if row['MiddleName'] == None:\n",
    "            doctor_info = \"{} {} {} {} {} {}\".format(row['FirstName'], row['LastName'], \n",
    "                                                  row['Primaryspecialty'], \n",
    "                                                  row['City'], row['State'],\n",
    "                                                  row['Zip.Code'])\n",
    "        else:\n",
    "            doctor_info = \"{} {} {} {} {} {} {}\".format(row['FirstName'], row['MiddleName'],  row['LastName'], \n",
    "                                                  row['Primaryspecialty'], \n",
    "                                                  row['City'], row['State'],\n",
    "                                                  row['Zip.Code'])\n",
    "            \n",
    "            \n",
    "        # print(doctor_info)\n",
    "        doctor_infolist.append([row['NPI'], doctor_info])\n",
    "\n",
    "    return doctor_infolist\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "DocListDF_path = 'DocListDF.p'\n",
    "\n",
    "df = pd.read_pickle(DocListDF_path) \n",
    "\n",
    "\n",
    "cols = ['cId', 'crawlFlag', 'NPI',  'LastName', 'FirstName',\n",
    "       'MiddleName',  'City', 'State', 'Zip.Code', 'Primaryspecialty']\n",
    "\n",
    "DoctorDF = df[cols]\n",
    "\n",
    "\n",
    "\n",
    "doctor_infolist = get_doctor_info(DoctorDF)\n",
    "\n",
    "\n",
    "len(doctor_infolist)\n",
    "\n",
    "\n",
    "\n",
    "start = 0\n",
    "length = 500\n",
    "\n",
    "end = start + length\n",
    "\n",
    "doctor_infolist = doctor_infolist[start: end]\n",
    "len(doctor_infolist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785ccbfb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T21:01:13.736668Z",
     "start_time": "2021-07-31T21:01:13.535392Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_doctor_info(DoctorDF):\n",
    "    doctor_infolist = []\n",
    "    for idx, row in DoctorDF.iterrows():\n",
    "\n",
    "        # most results: fistname is before lastname\n",
    "        if row['MiddleName'] == None:\n",
    "            doctor_info = \"{} {} {} {} {} {}\".format(row['FirstName'], row['LastName'], \n",
    "                                                  row['Primaryspecialty'], \n",
    "                                                  row['City'], row['State'],\n",
    "                                                  row['Zip.Code'])\n",
    "        else:\n",
    "            doctor_info = \"{} {} {} {} {} {} {}\".format(row['FirstName'], row['MiddleName'],  row['LastName'], \n",
    "                                                  row['Primaryspecialty'], \n",
    "                                                  row['City'], row['State'],\n",
    "                                                  row['Zip.Code'])\n",
    "            \n",
    "            \n",
    "        # print(doctor_info)\n",
    "        doctor_infolist.append([row['NPI'], doctor_info])\n",
    "\n",
    "    return doctor_infolist\n",
    "\n",
    "\n",
    "\n",
    "doctor_infolist = get_doctor_info(DoctorDF)\n",
    "\n",
    "\n",
    "\n",
    "doctor_infolist[:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0ad3d9",
   "metadata": {},
   "source": [
    "# Google Search URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be39db1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T21:14:13.107665Z",
     "start_time": "2021-07-31T21:14:13.075180Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "# from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "import requests\n",
    "from scrapy.http import TextResponse\n",
    "\n",
    "import cloudscraper\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# we need headers to disguise our bot as a browser\n",
    "\n",
    "HEADERS = {\n",
    "    \"Connection\": \"keep-alive\",\n",
    "    \"Cache-Control\": \"max-age=0\",\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
    "    # \"User-Agent\": \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/34.0.1847.131 Safari/537.36\",\n",
    "    'User-Agent': 'Mozilla/5.0 (X11; CrOS x86_64 12871.102.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.141 Safari/537.36',\n",
    "    \"Accept-Encoding\": \"gzip,deflate,sdch\",\n",
    "    # \"Accept-Language\": \"zh-CN,zh;q=0.8,en-US;q=0.6,en;q=0.4,zh-TW;q=0.2\",\n",
    "}\n",
    "\n",
    "HEADERS = {'User-Agent': 'Mozilla/5.0 (X11; CrOS x86_64 12871.102.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.141 Safari/537.36',}\n",
    "\n",
    "scraper = cloudscraper.create_scraper()\n",
    "\n",
    "for doctor_info in doctor_infolist:\n",
    "    NPI, keyword = doctor_info\n",
    "    # break\n",
    "\n",
    "    # print(keyword, NPI)\n",
    "    \n",
    "    \n",
    "page_number = 1\n",
    "print(NPI, keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12046891",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T21:14:13.757305Z",
     "start_time": "2021-07-31T21:14:13.753551Z"
    }
   },
   "outputs": [],
   "source": [
    "   \n",
    "# r = requests.get(url, headers=HEADERS, timeout=10)\n",
    "# r.url\n",
    "\n",
    "\n",
    "# load the text to scrapy-type response\n",
    "# response = TextResponse(r.url, body = r.text, encoding = 'utf-8')\n",
    "\n",
    "# have a look at its body\n",
    "# print(str(response.body.decode()))\n",
    "\n",
    "\n",
    "# with open('preview.html', 'w') as f:\n",
    "#     f.write(str(response.body.decode()))\n",
    "    \n",
    "# d = response.selector.xpath('.//div[@class=\"g\"]//div[@data-hveid]//a/@href').extract()\n",
    "# d = [i for i in d if 'http' in i]\n",
    "# print(len(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780319df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T21:14:16.428670Z",
     "start_time": "2021-07-31T21:14:14.483303Z"
    }
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "def get_doctor_google_result(keyword, pages = 2, timeout = 10):\n",
    "    \n",
    "    searched_urls = []\n",
    "    \n",
    "    # first two pages\n",
    "    for page_number in range(pages):\n",
    "        url = \"https://www.google.com/search?q=%s&start=%s\" % (keyword, (page_number)*10)\n",
    "        # print(url)\n",
    "        r = requests.get(url, headers=HEADERS, timeout=timeout)\n",
    "        \n",
    "        # try:\n",
    "        #     r = requests.get(url, headers=HEADERS, timeout=10)\n",
    "        # except:\n",
    "        #     r = scraper.get(url, headers=HEADERS, timeout=10)\n",
    "            \n",
    "            \n",
    "        response = TextResponse(r.url, body = r.text, encoding = 'utf-8')\n",
    "\n",
    "        # with open('preview.html', 'w') as f:\n",
    "        #     f.write(str(response.body.decode()))\n",
    "        \n",
    "        \n",
    "        d = response.selector.xpath('.//div[@class=\"g\"]//div[@data-hveid]//a/@href').extract()\n",
    "        # pprint(d)\n",
    "        d = [i for i in d if 'http' in i and 'googleusercontent' not in i and 'translate.google.com' not in i]\n",
    "        \n",
    "        print('From ULR {}'.format(url))\n",
    "        print('Get {} results'.format(len(d)))\n",
    "        searched_urls = searched_urls + d\n",
    "        \n",
    "    return searched_urls\n",
    "\n",
    "searched_urls = get_doctor_google_result(keyword, pages = 2)\n",
    "searched_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e582576",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b48d8d5",
   "metadata": {},
   "source": [
    "# Loop All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d23a9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "\n",
    "\n",
    "tmp_path = 'DocGoogleURL.p' # df.to_pickle()\n",
    "\n",
    "\n",
    "if os.path.exists(tmp_path):\n",
    "    collected_NPIs = pd.read_pickle(tmp_path)['NPI'].to_list()\n",
    "else:\n",
    "    collected_NPIs = []\n",
    "    \n",
    "print('Collected NPI {}'.format(len(collected_NPIs)))\n",
    "\n",
    "\n",
    "L = []\n",
    "for doctor_info in doctor_infolist:\n",
    "    \n",
    "    NPI, keyword = doctor_info\n",
    "    \n",
    "    if NPI in collected_NPIs:\n",
    "        continue\n",
    "        \n",
    "    \n",
    "    print('\\n\\n', str(datetime.now()), keyword, NPI)\n",
    "    \n",
    "    \n",
    "    try: \n",
    "        searched_urls = get_doctor_google_result(keyword, pages = 2)\n",
    "        status = 1\n",
    "        timestamp = str(datetime.now())\n",
    "        \n",
    "        \n",
    "        second = random.randrange(1, 5)\n",
    "        # time.sleep(second)\n",
    "    \n",
    "    except:\n",
    "        print('No result for:', NPI, keyword)\n",
    "        searched_urls = []\n",
    "        status = 0\n",
    "        timestamp = np.nan\n",
    "    \n",
    "    d = {\n",
    "        'NPI': NPI, \n",
    "        'status': status,\n",
    "        'searched_urls': searched_urls,\n",
    "        'clct_time': timestamp\n",
    "    }\n",
    "    \n",
    "    \n",
    "    L.append(d)\n",
    "    \n",
    "    pd.DataFrame(L).df.to_pickle('DocGoogleURL.p')\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8fcec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle('DocGoogleURL.p')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
